{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxPYJuz1EIXo",
        "outputId": "96cdf9b2-fc6d-4414-f402-38ab5c16ac31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Using GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Data Shape: (2358, 6)\n",
            "Test Data Shape: (800, 2)\n",
            "\n",
            "üîπ Evaluating Train Set...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict      0.000     0.000     0.000        33\n",
            "    negative      0.944     0.978     0.961       687\n",
            "     neutral      0.848     0.924     0.884       368\n",
            "    positive      0.969     0.939     0.954       798\n",
            "\n",
            "    accuracy                          0.934      1886\n",
            "   macro avg      0.690     0.710     0.700      1886\n",
            "weighted avg      0.919     0.934     0.926      1886\n",
            "\n",
            "Train Accuracy: 0.9337221633085896\n",
            "\n",
            "üîπ Evaluating Validation Set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict      0.000     0.000     0.000        12\n",
            "    negative      0.941     0.978     0.959       179\n",
            "     neutral      0.887     0.935     0.910        92\n",
            "    positive      0.958     0.958     0.958       189\n",
            "\n",
            "    accuracy                          0.936       472\n",
            "   macro avg      0.696     0.718     0.707       472\n",
            "weighted avg      0.913     0.936     0.925       472\n",
            "\n",
            "Validation Accuracy: 0.9364406779661016\n",
            "\n",
            "üîπ Evaluating Test Set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.000     0.000     0.000         0\n",
            "     neutral      1.000     0.691     0.817       800\n",
            "    positive      0.000     0.000     0.000         0\n",
            "\n",
            "    accuracy                          0.691       800\n",
            "   macro avg      0.333     0.230     0.272       800\n",
            "weighted avg      1.000     0.691     0.817       800\n",
            "\n",
            "Test Accuracy: 0.69125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# üìå Full ABSA Pipeline for Laptop Dataset (GPU enabled)\n",
        "# ============================================\n",
        "\n",
        "# Requirements:\n",
        "# pip install transformers torch huggingface_hub pandas scikit-learn\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import InferenceClient\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "use_api = False\n",
        "model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
        "hf_token_env = \"HF_TOKEN\"\n",
        "\n",
        "# ---------------------------\n",
        "# Device setup\n",
        "# ---------------------------\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# ---------------------------\n",
        "# Initialize model/client\n",
        "# ---------------------------\n",
        "if use_api:\n",
        "    if os.environ.get(hf_token_env) is None:\n",
        "        raise EnvironmentError(f\"Environment variable {hf_token_env} must be set for API mode\")\n",
        "    client = InferenceClient(api_key=os.environ[hf_token_env])\n",
        "    def classify_text(text):\n",
        "        resp = client.text_classification(model=model_name, inputs=text)\n",
        "        if isinstance(resp, list) and len(resp) > 0 and \"label\" in resp[0]:\n",
        "            return resp[0][\"label\"]\n",
        "        raise RuntimeError(\"Unexpected response from InferenceClient\")\n",
        "else:\n",
        "    pipe = pipeline(\"text-classification\", model=model_name, tokenizer=model_name, device=device)\n",
        "    def classify_text(text):\n",
        "        pred = pipe(text, truncation=True)\n",
        "        if isinstance(pred, list) and len(pred) > 0 and \"label\" in pred[0]:\n",
        "            return pred[0][\"label\"]\n",
        "        raise RuntimeError(\"Unexpected response from local pipeline\")\n",
        "\n",
        "# ---------------------------\n",
        "# Label Normalization\n",
        "# ---------------------------\n",
        "def normalize_label(label):\n",
        "    if not isinstance(label, str):\n",
        "        return \"neutral\"\n",
        "    l = label.strip().lower()\n",
        "    if l in (\"positive\", \"pos\", \"label_positive\"): return \"positive\"\n",
        "    if l in (\"negative\", \"neg\", \"label_negative\"): return \"negative\"\n",
        "    if l in (\"neutral\", \"neural\", \"label_neutral\"): return \"neutral\"\n",
        "    if l.startswith(\"label_\") and l[6:].isdigit():\n",
        "        idx = int(l[6:])\n",
        "        mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "        return mapping.get(idx, \"neutral\")\n",
        "    if \"pos\" in l: return \"positive\"\n",
        "    if \"neg\" in l: return \"negative\"\n",
        "    if \"neu\" in l: return \"neutral\"\n",
        "    return \"neutral\"\n",
        "\n",
        "# ---------------------------\n",
        "# ABSA for single review\n",
        "# ---------------------------\n",
        "def analyze_aspects(review, aspects_list):\n",
        "    results = {}\n",
        "    if not isinstance(review, str) or len(review.strip()) == 0:\n",
        "        return {a: \"neutral\" for a in aspects_list}\n",
        "    for asp in aspects_list:\n",
        "        text = f\"{review} [SEP] {asp}\"\n",
        "        raw_label = classify_text(text)\n",
        "        results[asp] = normalize_label(raw_label)\n",
        "    return results\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluate dataset\n",
        "# ---------------------------\n",
        "def evaluate_dataset(df, text_col=\"Sentence\", aspect_col=\"Aspect Term\", label_col=\"polarity\"):\n",
        "    \"\"\"\n",
        "    df must have columns: Sentence, Aspect Term, polarity\n",
        "    \"\"\"\n",
        "    y_true, y_pred = [], []\n",
        "    # Check if the required columns exist in the dataframe\n",
        "    if aspect_col not in df.columns or label_col not in df.columns:\n",
        "        print(f\"Warning: '{aspect_col}' or '{label_col}' column not found in the DataFrame. Skipping evaluation for this dataset.\")\n",
        "        return None # Or raise an error, depending on desired behavior\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        review, asp, true = row[text_col], row[aspect_col], row[label_col]\n",
        "        pred = analyze_aspects(review, [asp])[asp]\n",
        "        y_true.append(true.lower())\n",
        "        y_pred.append(pred)\n",
        "\n",
        "    # Handle the case where y_true is empty (no data to evaluate)\n",
        "    if not y_true:\n",
        "        print(\"Warning: No data to evaluate in the provided DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "# ---------------------------\n",
        "# Main pipeline\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Load datasets\n",
        "    train_df = pd.read_csv(\"/content/Laptop_Train_v2.csv\")\n",
        "    test_df = pd.read_csv(\"/content/Laptops_Test_Data_PhaseA.csv\")\n",
        "\n",
        "    print(\"\\nTrain Data Shape:\", train_df.shape)\n",
        "    print(\"Test Data Shape:\", test_df.shape)\n",
        "\n",
        "    # Split into train/val\n",
        "    train_split, val_split = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"\\nüîπ Evaluating Train Set...\")\n",
        "    train_acc = evaluate_dataset(train_split)\n",
        "    print(\"Train Accuracy:\", train_acc)\n",
        "\n",
        "    print(\"\\nüîπ Evaluating Validation Set...\")\n",
        "    val_acc = evaluate_dataset(val_split)\n",
        "    print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "    # Add missing columns to test_df for evaluation\n",
        "    if 'Aspect Term' not in test_df.columns:\n",
        "        test_df['Aspect Term'] = 'placeholder'  # Add a placeholder column\n",
        "    if 'polarity' not in test_df.columns:\n",
        "        test_df['polarity'] = 'neutral' # Add a placeholder column for evaluation, assuming 'neutral' as a default\n",
        "\n",
        "    print(\"\\nüîπ Evaluating Test Set...\")\n",
        "    test_acc = evaluate_dataset(test_df)\n",
        "    print(\"Test Accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict aspect terms and polarities for the test set\n",
        "test_predictions = []\n",
        "# Define aspects to analyze based on the training data or common laptop aspects\n",
        "# For demonstration, using a predefined list of common aspects\n",
        "aspects_to_analyze = [\n",
        "    \"battery\", \"performance\", \"display\", \"build\", \"value\",\n",
        "    \"keyboard\", \"sound\", \"weight\", \"design\", \"durability\",\n",
        "    \"cooling\", \"camera\", \"charging\", \"warranty\"\n",
        "]\n",
        "\n",
        "print(\"\\nüîπ Predicting aspects and sentiment for Test Set...\")\n",
        "for index, row in test_df.iterrows():\n",
        "    review = row['Sentence']\n",
        "    # Analyze aspects for each review in the test set\n",
        "    predicted_sentiments = analyze_aspects(review, aspects_to_analyze)\n",
        "    # Store the predictions. For simplicity, we can store the dictionary or process it further\n",
        "    # Here, we'll store the dictionary of predicted sentiments for each review\n",
        "    test_predictions.append(predicted_sentiments)\n",
        "\n",
        "# Add predictions to the test_df DataFrame\n",
        "# This will create a new column where each entry is a dictionary of aspect sentiments for that review\n",
        "test_df['Predicted_Aspect_Sentiments'] = test_predictions\n",
        "\n",
        "# Display the updated test_df with predictions\n",
        "print(\"\\nTest DataFrame with Predictions:\")\n",
        "display(test_df.head())\n",
        "\n",
        "# You can further process the 'Predicted_Aspect_Sentiments' column to extract specific aspect sentiments\n",
        "# For example, to see the predicted sentiment for 'battery' for each review:\n",
        "# test_df['Predicted_Battery_Sentiment'] = test_df['Predicted_Aspect_Sentiments'].apply(lambda x: x.get('battery', 'neutral'))\n",
        "# display(test_df[['Sentence', 'Predicted_Battery_Sentiment']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "wquARb9REbIA",
        "outputId": "63715593-e2c3-4de5-953a-66380d27a982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Predicting aspects and sentiment for Test Set...\n",
            "\n",
            "Test DataFrame with Predictions:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       id                                           Sentence  Aspect Term  \\\n",
              "0   892:1  Boot time is super fast, around anywhere from ...  placeholder   \n",
              "1  1144:1  tech support would not fix the problem unless ...  placeholder   \n",
              "2   805:2                 but in resume this computer rocks!  placeholder   \n",
              "3   359:1                                   Set up was easy.  placeholder   \n",
              "4   562:1  Did not enjoy the new Windows 8 and touchscree...  placeholder   \n",
              "\n",
              "  polarity                        Predicted_Aspect_Sentiments  \n",
              "0  neutral  {'battery': 'neutral', 'performance': 'positiv...  \n",
              "1  neutral  {'battery': 'neutral', 'performance': 'negativ...  \n",
              "2  neutral  {'battery': 'neutral', 'performance': 'positiv...  \n",
              "3  neutral  {'battery': 'positive', 'performance': 'positi...  \n",
              "4  neutral  {'battery': 'neutral', 'performance': 'negativ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2b8e946-c1b2-4cd5-9a95-ce6fc329bfff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Aspect Term</th>\n",
              "      <th>polarity</th>\n",
              "      <th>Predicted_Aspect_Sentiments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892:1</td>\n",
              "      <td>Boot time is super fast, around anywhere from ...</td>\n",
              "      <td>placeholder</td>\n",
              "      <td>neutral</td>\n",
              "      <td>{'battery': 'neutral', 'performance': 'positiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1144:1</td>\n",
              "      <td>tech support would not fix the problem unless ...</td>\n",
              "      <td>placeholder</td>\n",
              "      <td>neutral</td>\n",
              "      <td>{'battery': 'neutral', 'performance': 'negativ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>805:2</td>\n",
              "      <td>but in resume this computer rocks!</td>\n",
              "      <td>placeholder</td>\n",
              "      <td>neutral</td>\n",
              "      <td>{'battery': 'neutral', 'performance': 'positiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>359:1</td>\n",
              "      <td>Set up was easy.</td>\n",
              "      <td>placeholder</td>\n",
              "      <td>neutral</td>\n",
              "      <td>{'battery': 'positive', 'performance': 'positi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>562:1</td>\n",
              "      <td>Did not enjoy the new Windows 8 and touchscree...</td>\n",
              "      <td>placeholder</td>\n",
              "      <td>neutral</td>\n",
              "      <td>{'battery': 'neutral', 'performance': 'negativ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2b8e946-c1b2-4cd5-9a95-ce6fc329bfff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2b8e946-c1b2-4cd5-9a95-ce6fc329bfff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2b8e946-c1b2-4cd5-9a95-ce6fc329bfff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-87d1dd66-e30f-4d90-aa84-1729baa47d45\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-87d1dd66-e30f-4d90-aa84-1729baa47d45')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-87d1dd66-e30f-4d90-aa84-1729baa47d45 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# display(test_df[['Sentence', 'Predicted_Battery_Sentiment']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1144:1\",\n          \"562:1\",\n          \"805:2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"tech support would not fix the problem unless I bought your plan for $150 plus.\",\n          \"Did not enjoy the new Windows 8 and touchscreen functions.\",\n          \"but in resume this computer rocks!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Aspect Term\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"placeholder\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"polarity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicted_Aspect_Sentiments\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817bd008",
        "outputId": "be39c399-5acc-4f71-8211-b52ef05c259d"
      },
      "source": [
        "# Add missing columns to test_df for evaluation if they don't exist\n",
        "# These were added in a previous step, but this check ensures the code runs even if that step is skipped.\n",
        "if 'Aspect Term' not in test_df.columns:\n",
        "    test_df['Aspect Term'] = 'placeholder'  # Add a placeholder column\n",
        "if 'polarity' not in test_df.columns:\n",
        "    test_df['polarity'] = 'neutral' # Add a placeholder column for evaluation, assuming 'neutral' as a default\n",
        "\n",
        "print(\"\\nüîπ Evaluating Test Set...\")\n",
        "# Evaluate the test set using the evaluate_dataset function\n",
        "test_acc = evaluate_dataset(test_df)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Evaluating Test Set...\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.000     0.000     0.000         0\n",
            "     neutral      1.000     0.691     0.817       800\n",
            "    positive      0.000     0.000     0.000         0\n",
            "\n",
            "    accuracy                          0.691       800\n",
            "   macro avg      0.333     0.230     0.272       800\n",
            "weighted avg      1.000     0.691     0.817       800\n",
            "\n",
            "Test Accuracy: 0.69125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Improved Evaluate dataset\n",
        "# ---------------------------\n",
        "def evaluate_dataset(df, text_col=\"Sentence\", aspect_col=\"Aspect Term\", label_col=\"polarity\"):\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    if aspect_col not in df.columns or label_col not in df.columns:\n",
        "        print(f\"‚ö†Ô∏è Skipping evaluation: missing {aspect_col}/{label_col}. Running inference only...\")\n",
        "        df[\"predicted_polarity\"] = df[text_col].apply(lambda r: analyze_aspects(r, [\"general\"])[\"general\"])\n",
        "        return None\n",
        "\n",
        "    # Prepare batch inputs\n",
        "    texts, aspects, labels = [], [], []\n",
        "    for _, row in df.iterrows():\n",
        "        review, asp, true = row[text_col], row[aspect_col], row[label_col]\n",
        "        if pd.isna(review) or pd.isna(asp):\n",
        "            continue\n",
        "        texts.append(f\"{review} [SEP] {asp}\")\n",
        "        aspects.append(asp)\n",
        "        labels.append(true)\n",
        "\n",
        "    # Batch predictions\n",
        "    preds_raw = pipe(texts, truncation=True, batch_size=32)\n",
        "    preds = [normalize_label(p[\"label\"]) for p in preds_raw]\n",
        "\n",
        "    # Map conflict ‚Üí neutral\n",
        "    labels = [\"neutral\" if l.lower()==\"conflict\" else l.lower() for l in labels]\n",
        "\n",
        "    y_true.extend(labels)\n",
        "    y_pred.extend(preds)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=3))\n",
        "    return accuracy_score(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "JwVacluqJXAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"predicted_polarity\"] = test_df.apply(\n",
        "    lambda r: analyze_aspects(r[\"Sentence\"], [r.get(\"Aspect Term\", \"general\")]).get(r.get(\"Aspect Term\", \"general\"), \"neutral\"),\n",
        "    axis=1\n",
        ")\n",
        "test_df.to_csv(\"Laptop_Test_With_Predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "pV696LKVJZno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå ABSA Query-based Sentiment Analyzer\n",
        "# ============================================\n",
        "\n",
        "# Requirements:\n",
        "# pip install transformers torch huggingface_hub pandas\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# ---------------------------\n",
        "# Model & Pipeline\n",
        "# ---------------------------\n",
        "model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
        "pipe = pipeline(\"text-classification\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "def normalize_label(label):\n",
        "    l = label.strip().lower()\n",
        "    if \"positive\" in l or \"pos\" in l: return \"positive\"\n",
        "    if \"negative\" in l or \"neg\" in l: return \"negative\"\n",
        "    if \"neutral\" in l or \"neu\" in l: return \"neutral\"\n",
        "    if l.startswith(\"label_\"):\n",
        "        idx = int(l.split(\"_\")[-1])\n",
        "        mapping = {0:\"negative\", 1:\"neutral\", 2:\"positive\"}\n",
        "        return mapping.get(idx,\"neutral\")\n",
        "    return \"neutral\"\n",
        "\n",
        "# ---------------------------\n",
        "# Aspects to check\n",
        "# ---------------------------\n",
        "aspects = [\n",
        "    \"battery\", \"performance\", \"display\", \"build\", \"value\",\n",
        "    \"keyboard\", \"sound\", \"weight\", \"design\", \"durability\",\n",
        "    \"cooling\", \"camera\", \"charging\", \"warranty\"\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# ABSA for a single query\n",
        "# ---------------------------\n",
        "def analyze_query(query, aspects_list=aspects):\n",
        "    results = {}\n",
        "    for asp in aspects_list:\n",
        "        text = f\"{query} [SEP] {asp}\"\n",
        "        raw_label = pipe(text, truncation=True)[0][\"label\"]\n",
        "        results[asp] = normalize_label(raw_label)\n",
        "    return results\n",
        "\n",
        "# ---------------------------\n",
        "# Example usage\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    query1 = \"The battery lasts long but the display is too dim.\"\n",
        "    query2 = \"Performance is smooth, but the keyboard feels cheap.\"\n",
        "\n",
        "    for q in [query1, query2]:\n",
        "        print(f\"\\nüîπ Review: {q}\")\n",
        "        res = analyze_query(q)\n",
        "        for asp, sent in res.items():\n",
        "            if sent != \"neutral\":   # only print aspects that matter\n",
        "                print(f\"  Aspect: {asp:<12} ‚Üí Sentiment: {sent}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyaxpCm9EV8_",
        "outputId": "3573f18d-e16d-4686-a388-750e6ca53d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Review: The battery lasts long but the display is too dim.\n",
            "  Aspect: battery      ‚Üí Sentiment: positive\n",
            "  Aspect: performance  ‚Üí Sentiment: positive\n",
            "  Aspect: display      ‚Üí Sentiment: negative\n",
            "  Aspect: build        ‚Üí Sentiment: positive\n",
            "  Aspect: value        ‚Üí Sentiment: positive\n",
            "  Aspect: sound        ‚Üí Sentiment: positive\n",
            "  Aspect: weight       ‚Üí Sentiment: positive\n",
            "  Aspect: design       ‚Üí Sentiment: negative\n",
            "  Aspect: durability   ‚Üí Sentiment: positive\n",
            "  Aspect: camera       ‚Üí Sentiment: negative\n",
            "\n",
            "üîπ Review: Performance is smooth, but the keyboard feels cheap.\n",
            "  Aspect: battery      ‚Üí Sentiment: positive\n",
            "  Aspect: performance  ‚Üí Sentiment: positive\n",
            "  Aspect: display      ‚Üí Sentiment: positive\n",
            "  Aspect: build        ‚Üí Sentiment: positive\n",
            "  Aspect: value        ‚Üí Sentiment: positive\n",
            "  Aspect: keyboard     ‚Üí Sentiment: negative\n",
            "  Aspect: sound        ‚Üí Sentiment: positive\n",
            "  Aspect: weight       ‚Üí Sentiment: positive\n",
            "  Aspect: design       ‚Üí Sentiment: positive\n",
            "  Aspect: durability   ‚Üí Sentiment: positive\n",
            "  Aspect: cooling      ‚Üí Sentiment: positive\n",
            "  Aspect: camera       ‚Üí Sentiment: positive\n",
            "  Aspect: charging     ‚Üí Sentiment: positive\n",
            "  Aspect: warranty     ‚Üí Sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIfKcI8bHHMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìå Advanced ABSA Pipeline with Multiple Improvement Techniques\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "MODELS = [\n",
        "    \"yangheng/deberta-v3-base-absa-v1.1\",\n",
        "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "]\n",
        "PRIMARY_MODEL = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
        "\n",
        "# ---------------------------\n",
        "# Device setup\n",
        "# ---------------------------\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# ---------------------------\n",
        "# Enhanced Label Normalization\n",
        "# ---------------------------\n",
        "def normalize_label(label):\n",
        "    if not isinstance(label, str):\n",
        "        return \"neutral\"\n",
        "    l = label.strip().lower()\n",
        "\n",
        "    # Direct mappings\n",
        "    if l in (\"positive\", \"pos\", \"label_positive\", \"label_2\", \"5 stars\", \"4 stars\"):\n",
        "        return \"positive\"\n",
        "    if l in (\"negative\", \"neg\", \"label_negative\", \"label_0\", \"1 star\", \"2 stars\"):\n",
        "        return \"negative\"\n",
        "    if l in (\"neutral\", \"neural\", \"label_neutral\", \"label_1\", \"3 stars\"):\n",
        "        return \"neutral\"\n",
        "    if l in (\"conflict\", \"label_conflict\"):\n",
        "        return \"conflict\"\n",
        "\n",
        "    # Numeric label mapping\n",
        "    if l.startswith(\"label_\"):\n",
        "        if l[6:].isdigit():\n",
        "            idx = int(l[6:])\n",
        "            mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "            return mapping.get(idx, \"neutral\")\n",
        "\n",
        "    # Star ratings\n",
        "    if \"star\" in l:\n",
        "        if \"5\" in l or \"4\" in l: return \"positive\"\n",
        "        if \"1\" in l or \"2\" in l: return \"negative\"\n",
        "        if \"3\" in l: return \"neutral\"\n",
        "\n",
        "    # Substring matching\n",
        "    if \"pos\" in l: return \"positive\"\n",
        "    if \"neg\" in l: return \"negative\"\n",
        "    if \"neu\" in l: return \"neutral\"\n",
        "\n",
        "    return \"neutral\"\n",
        "\n",
        "# ---------------------------\n",
        "# Text Preprocessing and Augmentation\n",
        "# ---------------------------\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove special characters but keep sentence structure\n",
        "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def augment_text(text, aspect):\n",
        "    \"\"\"Generate augmented versions of text-aspect pairs\"\"\"\n",
        "    variations = [\n",
        "        f\"{text} [SEP] {aspect}\",\n",
        "        f\"Review: {text} [SEP] Aspect: {aspect}\",\n",
        "        f\"{text}. Opinion about {aspect}?\",\n",
        "        f\"What is the sentiment about {aspect} in: {text}\",\n",
        "    ]\n",
        "    return variations\n",
        "\n",
        "# ---------------------------\n",
        "# Multi-Model Ensemble Classifier\n",
        "# ---------------------------\n",
        "class EnsembleABSA:\n",
        "    def __init__(self, model_name, device):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.pipe = None\n",
        "        self._initialize_pipeline()\n",
        "\n",
        "    def _initialize_pipeline(self):\n",
        "        \"\"\"Initialize pipeline with error handling\"\"\"\n",
        "        try:\n",
        "            self.pipe = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=self.model_name,\n",
        "                tokenizer=self.model_name,\n",
        "                device=self.device,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "            print(f\"‚úÖ Loaded model: {self.model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load {self.model_name}: {e}\")\n",
        "            self.pipe = None\n",
        "\n",
        "    def predict(self, text, return_scores=False):\n",
        "        \"\"\"Get prediction with confidence scores\"\"\"\n",
        "        if self.pipe is None:\n",
        "            return \"neutral\", 0.33\n",
        "\n",
        "        try:\n",
        "            result = self.pipe(text, truncation=True, max_length=512)\n",
        "            if isinstance(result, list) and len(result) > 0:\n",
        "                label = normalize_label(result[0][\"label\"])\n",
        "                score = result[0].get(\"score\", 0.0)\n",
        "\n",
        "                if return_scores:\n",
        "                    return label, score\n",
        "                return label\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {e}\")\n",
        "            return \"neutral\", 0.33 if return_scores else \"neutral\"\n",
        "\n",
        "        return \"neutral\", 0.33 if return_scores else \"neutral\"\n",
        "\n",
        "# ---------------------------\n",
        "# Advanced ABSA with Multiple Strategies\n",
        "# ---------------------------\n",
        "class AdvancedABSA:\n",
        "    def __init__(self, primary_model, device, use_ensemble=True):\n",
        "        self.primary_classifier = EnsembleABSA(primary_model, device)\n",
        "        self.use_ensemble = use_ensemble\n",
        "        self.classifiers = []\n",
        "\n",
        "        if use_ensemble:\n",
        "            print(\"\\nüîÑ Initializing ensemble models...\")\n",
        "            for model in MODELS:\n",
        "                if model != primary_model:\n",
        "                    try:\n",
        "                        classifier = EnsembleABSA(model, device)\n",
        "                        if classifier.pipe is not None:\n",
        "                            self.classifiers.append(classifier)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "    def predict_with_voting(self, text, aspect):\n",
        "        \"\"\"Ensemble prediction with majority voting\"\"\"\n",
        "        predictions = []\n",
        "        scores = []\n",
        "\n",
        "        # Primary model prediction\n",
        "        label, score = self.primary_classifier.predict(text, return_scores=True)\n",
        "        predictions.append(label)\n",
        "        scores.append(score)\n",
        "\n",
        "        # Ensemble predictions\n",
        "        if self.use_ensemble:\n",
        "            for classifier in self.classifiers:\n",
        "                label, score = classifier.predict(text, return_scores=True)\n",
        "                predictions.append(label)\n",
        "                scores.append(score)\n",
        "\n",
        "        # Weighted voting based on confidence\n",
        "        if len(predictions) == 1:\n",
        "            return predictions[0]\n",
        "\n",
        "        # Majority voting with confidence weighting\n",
        "        weighted_votes = {}\n",
        "        for pred, score in zip(predictions, scores):\n",
        "            weighted_votes[pred] = weighted_votes.get(pred, 0) + score\n",
        "\n",
        "        return max(weighted_votes, key=weighted_votes.get)\n",
        "\n",
        "    def predict_with_augmentation(self, text, aspect):\n",
        "        \"\"\"Use text augmentation for robust prediction\"\"\"\n",
        "        text_clean = clean_text(text)\n",
        "        aspect_clean = clean_text(aspect)\n",
        "\n",
        "        # Generate variations\n",
        "        variations = augment_text(text_clean, aspect_clean)\n",
        "\n",
        "        predictions = []\n",
        "        for variation in variations:\n",
        "            pred = self.primary_classifier.predict(variation)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Majority vote\n",
        "        if predictions:\n",
        "            return Counter(predictions).most_common(1)[0][0]\n",
        "        return \"neutral\"\n",
        "\n",
        "    def predict_with_context_window(self, text, aspect):\n",
        "        \"\"\"Extract context around aspect for better prediction\"\"\"\n",
        "        text_clean = clean_text(text)\n",
        "        aspect_clean = clean_text(aspect)\n",
        "\n",
        "        # Try to find aspect in text\n",
        "        text_lower = text_clean.lower()\n",
        "        aspect_lower = aspect_clean.lower()\n",
        "\n",
        "        if aspect_lower in text_lower:\n",
        "            # Extract context window around aspect\n",
        "            idx = text_lower.find(aspect_lower)\n",
        "            start = max(0, idx - 100)\n",
        "            end = min(len(text_clean), idx + len(aspect_clean) + 100)\n",
        "            context = text_clean[start:end]\n",
        "\n",
        "            input_text = f\"{context} [SEP] {aspect_clean}\"\n",
        "        else:\n",
        "            input_text = f\"{text_clean} [SEP] {aspect_clean}\"\n",
        "\n",
        "        return self.primary_classifier.predict(input_text)\n",
        "\n",
        "    def predict_combined(self, text, aspect, strategy=\"voting\"):\n",
        "        \"\"\"Combined prediction using multiple strategies\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        if strategy == \"all\" or strategy == \"voting\":\n",
        "            pred = self.predict_with_voting(\n",
        "                f\"{clean_text(text)} [SEP] {clean_text(aspect)}\",\n",
        "                aspect\n",
        "            )\n",
        "            predictions.append(pred)\n",
        "\n",
        "        if strategy == \"all\" or strategy == \"augmentation\":\n",
        "            pred = self.predict_with_augmentation(text, aspect)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        if strategy == \"all\" or strategy == \"context\":\n",
        "            pred = self.predict_with_context_window(text, aspect)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Final majority vote\n",
        "        if predictions:\n",
        "            return Counter(predictions).most_common(1)[0][0]\n",
        "        return \"neutral\"\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation with Advanced Techniques\n",
        "# ---------------------------\n",
        "def evaluate_with_strategy(df, absa_model, strategy=\"voting\",\n",
        "                          text_col=\"Sentence\", aspect_col=\"Aspect Term\",\n",
        "                          label_col=\"polarity\", dataset_name=\"Dataset\"):\n",
        "    \"\"\"Evaluate using specified strategy\"\"\"\n",
        "\n",
        "    if aspect_col not in df.columns or label_col not in df.columns:\n",
        "        print(f\"‚ö†Ô∏è  Required columns missing in {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä {dataset_name} - Strategy: {strategy.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    total = len(df)\n",
        "    for idx, row in df.iterrows():\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"Processing: {idx + 1}/{total}\", end='\\r')\n",
        "\n",
        "        try:\n",
        "            text = str(row[text_col])\n",
        "            aspect = str(row[aspect_col])\n",
        "            true_label = normalize_label(str(row[label_col]))\n",
        "\n",
        "            if pd.isna(text) or pd.isna(aspect) or not text.strip() or not aspect.strip():\n",
        "                continue\n",
        "\n",
        "            # Get prediction using specified strategy\n",
        "            pred = absa_model.predict_combined(text, aspect, strategy=strategy)\n",
        "\n",
        "            y_true.append(true_label)\n",
        "            y_pred.append(pred)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    if not y_true:\n",
        "        print(f\"\\n‚ö†Ô∏è  No valid data in {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nProcessed: {len(y_true)}/{total} samples\")\n",
        "\n",
        "    labels = sorted(list(set(y_true)))\n",
        "\n",
        "    print(f\"\\nüìà Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, labels=labels, digits=4, zero_division=0))\n",
        "\n",
        "    print(f\"\\nüéØ Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    print(f\"Labels: {labels}\")\n",
        "    print(cm)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, labels=labels, average='macro', zero_division=0)\n",
        "    f1_weighted = f1_score(y_true, y_pred, labels=labels, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"\\n‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"üìä F1-Score (Macro): {f1_macro:.4f}\")\n",
        "    print(f\"üìä F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'strategy': strategy\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Cross-Validation for Robustness\n",
        "# ---------------------------\n",
        "def cross_validate_model(df, absa_model, n_splits=5):\n",
        "    \"\"\"Perform stratified k-fold cross-validation\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîÑ Performing {n_splits}-Fold Cross-Validation\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['polarity']), 1):\n",
        "        print(f\"\\nFold {fold}/{n_splits}\")\n",
        "        val_fold = df.iloc[val_idx]\n",
        "\n",
        "        result = evaluate_with_strategy(\n",
        "            val_fold,\n",
        "            absa_model,\n",
        "            strategy=\"voting\",\n",
        "            dataset_name=f\"Fold {fold}\"\n",
        "        )\n",
        "\n",
        "        if result:\n",
        "            scores.append(result['accuracy'])\n",
        "\n",
        "    if scores:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Cross-Validation Results:\")\n",
        "        print(f\"Mean Accuracy: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# Main Pipeline\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"üöÄ Advanced ABSA Pipeline Starting\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"\\nüìÅ Loading datasets...\")\n",
        "    train_df = pd.read_csv(\"/content/Laptop_Train_v2.csv\")\n",
        "    test_df = pd.read_csv(\"/content/Laptops_Test_Data_PhaseA.csv\")\n",
        "\n",
        "    print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
        "\n",
        "    # Preprocess\n",
        "    train_df = train_df.dropna(subset=['Sentence', 'Aspect Term', 'polarity'])\n",
        "    train_df['polarity'] = train_df['polarity'].apply(normalize_label)\n",
        "\n",
        "    # Initialize Advanced ABSA\n",
        "    print(\"\\nü§ñ Initializing Advanced ABSA System...\")\n",
        "    absa = AdvancedABSA(PRIMARY_MODEL, device, use_ensemble=False)\n",
        "\n",
        "    # Split data\n",
        "    train_split, val_split = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['polarity'])\n",
        "\n",
        "    # Evaluate with different strategies\n",
        "    strategies = [\"voting\", \"augmentation\", \"context\", \"all\"]\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä VALIDATION SET - Testing Different Strategies\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for strategy in strategies:\n",
        "        result = evaluate_with_strategy(\n",
        "            val_split,\n",
        "            absa,\n",
        "            strategy=strategy,\n",
        "            dataset_name=f\"Validation ({strategy})\"\n",
        "        )\n",
        "        if result:\n",
        "            results[strategy] = result\n",
        "\n",
        "    # Find best strategy\n",
        "    if results:\n",
        "        best_strategy = max(results, key=lambda x: results[x]['accuracy'])\n",
        "        print(f\"\\nüèÜ Best Strategy: {best_strategy.upper()} - Accuracy: {results[best_strategy]['accuracy']:.4f}\")\n",
        "    else:\n",
        "        best_strategy = \"voting\"\n",
        "\n",
        "    # Evaluate test set with best strategy\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ TEST SET EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if 'Aspect Term' in test_df.columns and 'polarity' in test_df.columns:\n",
        "        test_df = test_df.dropna(subset=['Sentence', 'Aspect Term', 'polarity'])\n",
        "        test_df['polarity'] = test_df['polarity'].apply(normalize_label)\n",
        "\n",
        "        test_result = evaluate_with_strategy(\n",
        "            test_df,\n",
        "            absa,\n",
        "            strategy=best_strategy,\n",
        "            dataset_name=\"Test Set\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Test set is unlabeled - cannot evaluate\")\n",
        "\n",
        "    # Optional: Cross-validation\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîÑ Optional: Cross-Validation Analysis\")\n",
        "    print(\"=\"*60)\n",
        "    choice = input(\"Run cross-validation? (y/n): \").lower()\n",
        "    if choice == 'y':\n",
        "        cv_scores = cross_validate_model(train_df, absa, n_splits=5)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úÖ PIPELINE COMPLETE\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klxssF7VM1pe",
        "outputId": "d27f3d88-d7f4-43c5-ba8e-fbabab0fa301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Using GPU: Tesla T4\n",
            "============================================================\n",
            "üöÄ Advanced ABSA Pipeline Starting\n",
            "============================================================\n",
            "\n",
            "üìÅ Loading datasets...\n",
            "Train: (2358, 6), Test: (800, 2)\n",
            "\n",
            "ü§ñ Initializing Advanced ABSA System...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded model: yangheng/deberta-v3-base-absa-v1.1\n",
            "\n",
            "============================================================\n",
            "üìä VALIDATION SET - Testing Different Strategies\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "üìä Validation (voting) - Strategy: VOTING\n",
            "============================================================\n",
            "Processing: 1400/472\n",
            "Processed: 472/472 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9382    0.9653    0.9516       173\n",
            "     neutral     0.8500    0.9239    0.8854        92\n",
            "    positive     0.9691    0.9495    0.9592       198\n",
            "\n",
            "    accuracy                         0.9322       472\n",
            "   macro avg     0.6893    0.7097    0.6990       472\n",
            "weighted avg     0.9161    0.9322    0.9237       472\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   6   2   1]\n",
            " [  0 167   4   2]\n",
            " [  0   4  85   3]\n",
            " [  0   1   9 188]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9322\n",
            "üìä F1-Score (Macro): 0.6990\n",
            "üìä F1-Score (Weighted): 0.9237\n",
            "\n",
            "============================================================\n",
            "üìä Validation (augmentation) - Strategy: AUGMENTATION\n",
            "============================================================\n",
            "\n",
            "Processed: 472/472 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9379    0.9595    0.9486       173\n",
            "     neutral     0.8763    0.9239    0.8995        92\n",
            "    positive     0.9646    0.9646    0.9646       198\n",
            "\n",
            "    accuracy                         0.9364       472\n",
            "   macro avg     0.6947    0.7120    0.7032       472\n",
            "weighted avg     0.9192    0.9364    0.9277       472\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   6   2   1]\n",
            " [  0 166   4   3]\n",
            " [  0   4  85   3]\n",
            " [  0   1   6 191]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9364\n",
            "üìä F1-Score (Macro): 0.7032\n",
            "üìä F1-Score (Weighted): 0.9277\n",
            "\n",
            "============================================================\n",
            "üìä Validation (context) - Strategy: CONTEXT\n",
            "============================================================\n",
            "Processing: 1400/472\n",
            "Processed: 472/472 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9375    0.9538    0.9456       173\n",
            "     neutral     0.8571    0.9130    0.8842        92\n",
            "    positive     0.9545    0.9545    0.9545       198\n",
            "\n",
            "    accuracy                         0.9280       472\n",
            "   macro avg     0.6873    0.7053    0.6961       472\n",
            "weighted avg     0.9111    0.9280    0.9193       472\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   6   2   1]\n",
            " [  0 165   4   4]\n",
            " [  0   4  84   4]\n",
            " [  0   1   8 189]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9280\n",
            "üìä F1-Score (Macro): 0.6961\n",
            "üìä F1-Score (Weighted): 0.9193\n",
            "\n",
            "============================================================\n",
            "üìä Validation (all) - Strategy: ALL\n",
            "============================================================\n",
            "\n",
            "Processed: 472/472 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9382    0.9653    0.9516       173\n",
            "     neutral     0.8500    0.9239    0.8854        92\n",
            "    positive     0.9691    0.9495    0.9592       198\n",
            "\n",
            "    accuracy                         0.9322       472\n",
            "   macro avg     0.6893    0.7097    0.6990       472\n",
            "weighted avg     0.9161    0.9322    0.9237       472\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   6   2   1]\n",
            " [  0 167   4   2]\n",
            " [  0   4  85   3]\n",
            " [  0   1   9 188]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9322\n",
            "üìä F1-Score (Macro): 0.6990\n",
            "üìä F1-Score (Weighted): 0.9237\n",
            "\n",
            "üèÜ Best Strategy: AUGMENTATION - Accuracy: 0.9364\n",
            "\n",
            "============================================================\n",
            "üéØ TEST SET EVALUATION\n",
            "============================================================\n",
            "‚ö†Ô∏è  Test set is unlabeled - cannot evaluate\n",
            "\n",
            "============================================================\n",
            "üîÑ Optional: Cross-Validation Analysis\n",
            "============================================================\n",
            "Run cross-validation? (y/n): y\n",
            "\n",
            "============================================================\n",
            "üîÑ Performing 5-Fold Cross-Validation\n",
            "============================================================\n",
            "\n",
            "Fold 1/5\n",
            "\n",
            "============================================================\n",
            "üìä Fold 1 - Strategy: VOTING\n",
            "============================================================\n",
            "\n",
            "Processed: 472/472 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9235    0.9769    0.9494       173\n",
            "     neutral     0.9111    0.8913    0.9011        92\n",
            "    positive     0.9648    0.9697    0.9673       198\n",
            "\n",
            "    accuracy                         0.9386       472\n",
            "   macro avg     0.6999    0.7095    0.7044       472\n",
            "weighted avg     0.9208    0.9386    0.9294       472\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   4   2   3]\n",
            " [  0 169   1   3]\n",
            " [  0   9  82   1]\n",
            " [  0   1   5 192]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9386\n",
            "üìä F1-Score (Macro): 0.7044\n",
            "üìä F1-Score (Weighted): 0.9294\n",
            "\n",
            "Fold 2/5\n",
            "\n",
            "============================================================\n",
            "üìä Fold 2 - Strategy: VOTING\n",
            "============================================================\n",
            "\n",
            "Processed: 472/472 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9548    0.9769    0.9657       173\n",
            "     neutral     0.8416    0.9239    0.8808        92\n",
            "    positive     0.9588    0.9394    0.9490       198\n",
            "\n",
            "    accuracy                         0.9322       472\n",
            "   macro avg     0.6888    0.7100    0.6989       472\n",
            "weighted avg     0.9162    0.9322    0.9237       472\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   4   2   3]\n",
            " [  0 169   2   2]\n",
            " [  0   4  85   3]\n",
            " [  0   0  12 186]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9322\n",
            "üìä F1-Score (Macro): 0.6989\n",
            "üìä F1-Score (Weighted): 0.9237\n",
            "\n",
            "Fold 3/5\n",
            "\n",
            "============================================================\n",
            "üìä Fold 3 - Strategy: VOTING\n",
            "============================================================\n",
            "\n",
            "Processed: 472/472 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9341    0.9770    0.9551       174\n",
            "     neutral     0.8447    0.9457    0.8923        92\n",
            "    positive     0.9733    0.9239    0.9479       197\n",
            "\n",
            "    accuracy                         0.9301       472\n",
            "   macro avg     0.6880    0.7116    0.6988       472\n",
            "weighted avg     0.9152    0.9301    0.9216       472\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   7   0   2]\n",
            " [  0 170   3   1]\n",
            " [  0   3  87   2]\n",
            " [  0   2  13 182]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9301\n",
            "üìä F1-Score (Macro): 0.6988\n",
            "üìä F1-Score (Weighted): 0.9216\n",
            "\n",
            "Fold 4/5\n",
            "\n",
            "============================================================\n",
            "üìä Fold 4 - Strategy: VOTING\n",
            "============================================================\n",
            "\n",
            "Processed: 471/471 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9497    0.9827    0.9659       173\n",
            "     neutral     0.8673    0.9239    0.8947        92\n",
            "    positive     0.9639    0.9492    0.9565       197\n",
            "\n",
            "    accuracy                         0.9384       471\n",
            "   macro avg     0.6952    0.7140    0.7043       471\n",
            "weighted avg     0.9214    0.9384    0.9296       471\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   5   1   3]\n",
            " [  0 170   2   1]\n",
            " [  0   4  85   3]\n",
            " [  0   0  10 187]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9384\n",
            "üìä F1-Score (Macro): 0.7043\n",
            "üìä F1-Score (Weighted): 0.9296\n",
            "\n",
            "Fold 5/5\n",
            "\n",
            "============================================================\n",
            "üìä Fold 5 - Strategy: VOTING\n",
            "============================================================\n",
            "\n",
            "Processed: 471/471 samples\n",
            "\n",
            "üìà Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    conflict     0.0000    0.0000    0.0000         9\n",
            "    negative     0.9543    0.9653    0.9598       173\n",
            "     neutral     0.8131    0.9457    0.8744        92\n",
            "    positive     0.9630    0.9239    0.9430       197\n",
            "\n",
            "    accuracy                         0.9257       471\n",
            "   macro avg     0.6826    0.7087    0.6943       471\n",
            "weighted avg     0.9121    0.9257    0.9177       471\n",
            "\n",
            "\n",
            "üéØ Confusion Matrix:\n",
            "Labels: ['conflict', 'negative', 'neutral', 'positive']\n",
            "[[  0   5   1   3]\n",
            " [  0 167   5   1]\n",
            " [  0   2  87   3]\n",
            " [  0   1  14 182]]\n",
            "\n",
            "‚úÖ Accuracy: 0.9257\n",
            "üìä F1-Score (Macro): 0.6943\n",
            "üìä F1-Score (Weighted): 0.9177\n",
            "\n",
            "============================================================\n",
            "Cross-Validation Results:\n",
            "Mean Accuracy: 0.9330 (+/- 0.0050)\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "‚úÖ PIPELINE COMPLETE\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-gZn1yqOe3t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}